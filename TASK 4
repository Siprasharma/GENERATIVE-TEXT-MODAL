# Implement text generation
def generate_text(model, tokenizer, prompt, max_length=200, num_return_sequences=1):
    """Generates text based on a prompt."""
    inputs = tokenizer(prompt, return_tensors="pt")
    # Ensure inputs are on the same device as the model
    inputs = {key: value.to(model.device) for key, value in inputs.items()}
    outputs = model.generate(
        inputs["input_ids"],
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        no_repeat_ngram_size=2,
        early_stopping=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id,
    )
    generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return generated_text

# You can test the generation here after training
# prompt = "The arena was a dangerous place"
# generated_text = generate_text(model, tokenizer, prompt)
# print(generated_text[0])
 Demonstrate text generation
prompt = "The arena was a dangerous place" # @param {type:"string"}
generated_text = generate_text(model, tokenizer, prompt)

print("Generated Text:")
print(generated_text[0])
